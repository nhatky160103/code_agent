# Setup Guide - S·ª≠ d·ª•ng c√°c Module M·ªõi

H∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ setup v√† s·ª≠ d·ª•ng t·∫•t c·∫£ c√°c module m·ªõi ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p.

## üì¶ B∆∞·ªõc 1: C√†i ƒë·∫∑t Dependencies

C√°c module m·ªõi y√™u c·∫ßu th√™m m·ªôt s·ªë packages. Ch·∫°y l·ªánh sau:

```bash
pip install -r requirements.txt
```

Ho·∫∑c c√†i ƒë·∫∑t t·ª´ng package n·∫øu c·∫ßn:

```bash
# Caching
pip install diskcache>=5.6.0

# Logging & Metrics
pip install structlog>=23.2.0
pip install prometheus-client>=0.19.0
pip install rich>=13.7.0

# Rate Limiting & Resilience
pip install tenacity>=8.2.0
pip install circuitbreaker>=1.4.0

# Configuration
pip install pydantic-settings>=2.1.0
```

## üîë B∆∞·ªõc 2: C·∫•u h√¨nh .env File

T·∫°o ho·∫∑c c·∫≠p nh·∫≠t file `.env` trong th∆∞ m·ª•c g·ªëc c·ªßa project:

```env
# ============================================
# API Keys (B·∫ÆT BU·ªòC)
# ============================================
OPENROUTER_API_KEY=sk-or-v1-your-key-here
# Ho·∫∑c s·ª≠ d·ª•ng Google Gemini (t√πy ch·ªçn)
# GOOGLE_API_KEY=your-google-api-key

# ============================================
# Logging Configuration
# ============================================
LOG_LEVEL=INFO
# C√°c m·ª©c: DEBUG, INFO, WARNING, ERROR, CRITICAL

# Enable Prometheus metrics server (t√πy ch·ªçn)
ENABLE_METRICS_SERVER=false
METRICS_PORT=8000
# N·∫øu b·∫≠t, metrics s·∫Ω c√≥ s·∫µn t·∫°i http://localhost:8000/metrics

# ============================================
# Caching Configuration
# ============================================
CACHE_ENABLED=true
CACHE_TTL=3600
# TTL t√≠nh b·∫±ng gi√¢y (3600 = 1 gi·ªù)
# Cache s·∫Ω gi·∫£m API calls ƒë√°ng k·ªÉ, ti·∫øt ki·ªám chi ph√≠

CACHE_SIZE_LIMIT=1073741824
# Gi·ªõi h·∫°n cache size (bytes), m·∫∑c ƒë·ªãnh 1GB
# 1073741824 = 1GB

# ============================================
# Rate Limiting Configuration
# ============================================
RATE_LIMIT_ENABLED=true
RATE_LIMIT_MAX_CALLS=60
# S·ªë l∆∞·ª£ng calls t·ªëi ƒëa trong m·ªôt period
RATE_LIMIT_PERIOD=60
# Period t√≠nh b·∫±ng gi√¢y (60 = 1 ph√∫t)

# ============================================
# Retry Configuration
# ============================================
MAX_RETRIES=3
# S·ªë l·∫ßn retry t·ªëi ƒëa khi g·∫∑p l·ªói

RETRY_INITIAL_WAIT=1.0
# Th·ªùi gian ch·ªù ban ƒë·∫ßu (gi√¢y) tr∆∞·ªõc khi retry

RETRY_MAX_WAIT=60.0
# Th·ªùi gian ch·ªù t·ªëi ƒëa (gi√¢y) gi·ªØa c√°c l·∫ßn retry

RETRY_EXPONENTIAL_BASE=2.0
# H·ªá s·ªë exponential backoff (2.0 = nh√¢n ƒë√¥i m·ªói l·∫ßn)

ENABLE_CIRCUIT_BREAKER=true
# B·∫≠t circuit breaker ƒë·ªÉ tr√°nh cascade failures

# ============================================
# LLM Configuration
# ============================================
DEFAULT_MODEL=code
# Lo·∫°i model m·∫∑c ƒë·ªãnh: code, general, fast

TEMPERATURE=0.7
# Nhi·ªát ƒë·ªô cho LLM (0.0 - 1.0)

MAX_TOKENS=2000
# S·ªë tokens t·ªëi ƒëa m·ªói request

# ============================================
# Workspace Configuration
# ============================================
WORKSPACE_PATH=.
# ƒê∆∞·ªùng d·∫´n workspace (m·∫∑c ƒë·ªãnh: th∆∞ m·ª•c hi·ªán t·∫°i)
```

## üöÄ B∆∞·ªõc 3: Ch·∫°y H·ªá Th·ªëng

### C√°ch 1: Ch·∫°y CLI (ƒê∆°n gi·∫£n nh·∫•t)

```bash
python main.py "analyze codebase"
```

T·∫•t c·∫£ c√°c module s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c k√≠ch ho·∫°t:
- ‚úÖ Caching t·ª± ƒë·ªông cache LLM responses
- ‚úÖ Structured logging ghi v√†o `logs/YYYY-MM-DD.log`
- ‚úÖ Metrics ƒë∆∞·ª£c track (n·∫øu b·∫≠t metrics server)
- ‚úÖ Rate limiting t·ª± ƒë·ªông √°p d·ª•ng
- ‚úÖ Retry logic t·ª± ƒë·ªông x·ª≠ l√Ω errors

### C√°ch 2: Ki·ªÉm tra Metrics (N·∫øu ƒë√£ b·∫≠t)

N·∫øu b·∫°n ƒë√£ set `ENABLE_METRICS_SERVER=true` trong `.env`:

1. Ch·∫°y workflow:
```bash
python main.py "analyze codebase"
```

2. M·ªü browser v√† truy c·∫≠p:
```
http://localhost:8000/metrics
```

B·∫°n s·∫Ω th·∫•y Prometheus metrics nh∆∞:
```
llm_requests_total{agent="coder",model="gemini-2.5-flash",status="success"} 5.0
llm_request_duration_seconds_bucket{agent="coder",model="gemini-2.5-flash",le="1.0"} 3.0
workflow_duration_seconds_sum{status="success"} 45.2
```

## üìä B∆∞·ªõc 4: Ki·ªÉm tra Logs

Logs ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông trong th∆∞ m·ª•c `logs/`:

```bash
# Xem log h√¥m nay
cat logs/$(date +%Y-%m-%d).log

# Ho·∫∑c tail ƒë·ªÉ xem real-time
tail -f logs/$(date +%Y-%m-%d).log
```

Logs c√≥ format JSON, d·ªÖ parse:
```json
{
  "event": "llm_request_started",
  "agent": "coder",
  "model": "gemini-2.5-flash",
  "prompt_length": 1234,
  "timestamp": "2024-01-15T10:30:45.123456"
}
```

## üíæ B∆∞·ªõc 5: Ki·ªÉm tra Cache

Cache ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông trong th∆∞ m·ª•c `.cache/`:

```bash
# Xem cache directory
ls -lh .cache/

# X√≥a cache n·∫øu c·∫ßn
rm -rf .cache/
```

Cache s·∫Ω t·ª± ƒë·ªông:
- Gi·∫£m API calls cho prompts t∆∞∆°ng t·ª±
- TƒÉng t·ªëc ƒë·ªô response (10-100x cho cached requests)
- Ti·∫øt ki·ªám chi ph√≠ API

## ‚öôÔ∏è B∆∞·ªõc 6: T√πy ch·ªânh C·∫•u h√¨nh

### T·∫Øt Caching (N·∫øu c·∫ßn)

Trong `.env`:
```env
CACHE_ENABLED=false
```

Ho·∫∑c trong code:
```python
from utils.cache import get_cache
cache = get_cache()
cache.disable()
```

### Thay ƒë·ªïi Log Level

Trong `.env`:
```env
LOG_LEVEL=DEBUG  # ƒê·ªÉ xem chi ti·∫øt h∆°n
```

### T·∫Øt Rate Limiting

Trong `.env`:
```env
RATE_LIMIT_ENABLED=false
```

## üîç B∆∞·ªõc 7: Verify Setup

Ch·∫°y test script ƒë·ªÉ ki·ªÉm tra:

```bash
python test_api.py
```

N·∫øu m·ªçi th·ª© OK, b·∫°n s·∫Ω th·∫•y:
- ‚úÖ API connection successful
- ‚úÖ Models fetched successfully
- ‚úÖ Chat completion works

## üìù V√≠ d·ª• S·ª≠ D·ª•ng

### Example 1: Ch·∫°y v·ªõi Caching

```bash
# L·∫ßn ƒë·∫ßu - s·∫Ω g·ªçi API
python main.py "analyze codebase"

# L·∫ßn th·ª© hai v·ªõi prompt t∆∞∆°ng t·ª± - s·∫Ω d√πng cache (nhanh h∆°n nhi·ªÅu)
python main.py "analyze codebase"
```

### Example 2: Xem Metrics

```bash
# B·∫≠t metrics server trong .env
ENABLE_METRICS_SERVER=true

# Ch·∫°y workflow
python main.py "build a todo app"

# Xem metrics
curl http://localhost:8000/metrics | grep llm_requests_total
```

### Example 3: Debug Mode

```bash
# Set log level = DEBUG trong .env
LOG_LEVEL=DEBUG

# Ch·∫°y v√† xem chi ti·∫øt logs
python main.py "fix bugs" | tail -f logs/$(date +%Y-%m-%d).log
```

## üêõ Troubleshooting

### L·ªói: "diskcache is required for caching"

```bash
pip install diskcache
```

### L·ªói: "structlog could not be resolved"

```bash
pip install structlog prometheus-client
```

### Metrics server kh√¥ng start

- Ki·ªÉm tra port 8000 c√≥ ƒëang ƒë∆∞·ª£c d√πng kh√¥ng
- Th·ª≠ ƒë·ªïi `METRICS_PORT=8001` trong `.env`

### Cache kh√¥ng ho·∫°t ƒë·ªông

- Ki·ªÉm tra `CACHE_ENABLED=true` trong `.env`
- Ki·ªÉm tra quy·ªÅn ghi v√†o th∆∞ m·ª•c `.cache/`

### Rate limit v·∫´n b·ªã l·ªói

- TƒÉng `RATE_LIMIT_PERIOD` l√™n 120 (2 ph√∫t)
- Gi·∫£m `RATE_LIMIT_MAX_CALLS` xu·ªëng 30

## ‚úÖ Checklist Setup

- [ ] ƒê√£ c√†i ƒë·∫∑t t·∫•t c·∫£ dependencies (`pip install -r requirements.txt`)
- [ ] ƒê√£ t·∫°o file `.env` v·ªõi `OPENROUTER_API_KEY`
- [ ] ƒê√£ c·∫•u h√¨nh c√°c settings c·∫ßn thi·∫øt trong `.env`
- [ ] ƒê√£ test ch·∫°y `python main.py "test"` th√†nh c√¥ng
- [ ] ƒê√£ ki·ªÉm tra logs trong `logs/` directory
- [ ] (T√πy ch·ªçn) ƒê√£ b·∫≠t metrics server v√† ki·ªÉm tra t·∫°i `http://localhost:8000/metrics`

## üéØ Quick Start (Minimal)

N·∫øu b·∫°n ch·ªâ mu·ªën ch·∫°y nhanh v·ªõi c·∫•u h√¨nh t·ªëi thi·ªÉu:

1. **C√†i dependencies:**
```bash
pip install -r requirements.txt
```

2. **T·∫°o `.env` v·ªõi API key:**
```env
OPENROUTER_API_KEY=sk-or-v1-your-key
```

3. **Ch·∫°y:**
```bash
python main.py "analyze codebase"
```

T·∫•t c·∫£ c√°c module s·∫Ω t·ª± ƒë·ªông ho·∫°t ƒë·ªông v·ªõi default settings!

## üìö T√†i li·ªáu Th√™m

- `INTEGRATION_RECOMMENDATIONS.md` - Chi ti·∫øt v·ªÅ c√°c module
- `IMPLEMENTATION_SUMMARY.md` - T√≥m t·∫Øt implementation
- `FIXES_APPLIED.md` - C√°c fixes ƒë√£ √°p d·ª•ng

